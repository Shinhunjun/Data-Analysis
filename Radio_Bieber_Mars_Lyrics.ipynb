{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shinhunjun/Data-Analysis_Study/blob/main/Radio_Bieber_Mars_Lyrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "u7Wkp0sJHlvG",
        "outputId": "8dc687ca-be99-4030-f81f-5cf24ea9f4f8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7845874a-f12b-427f-9900-0c8f4b696876\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7845874a-f12b-427f-9900-0c8f4b696876\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving Radio_Bieber_Mars.txt to Radio_Bieber_Mars.txt\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_y4f7lJ4KOGQ",
        "outputId": "958dff67-50f7-4877-aa6f-f4412819fdc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.213325 M parameters\n",
            "step 0: train loss 4.4312, val loss 4.4320\n",
            "step 100: train loss 2.5298, val loss 2.5542\n",
            "step 200: train loss 2.4180, val loss 2.4540\n",
            "step 300: train loss 2.3539, val loss 2.4031\n",
            "step 400: train loss 2.3012, val loss 2.3557\n",
            "step 500: train loss 2.2622, val loss 2.3136\n",
            "step 600: train loss 2.2014, val loss 2.2637\n",
            "step 700: train loss 2.1266, val loss 2.1982\n",
            "step 800: train loss 2.0557, val loss 2.1388\n",
            "step 900: train loss 2.0102, val loss 2.0850\n",
            "step 1000: train loss 1.9649, val loss 2.0446\n",
            "step 1100: train loss 1.9201, val loss 1.9986\n",
            "step 1200: train loss 1.8787, val loss 1.9598\n",
            "step 1300: train loss 1.8409, val loss 1.9316\n",
            "step 1400: train loss 1.8207, val loss 1.9145\n",
            "step 1500: train loss 1.7896, val loss 1.8896\n",
            "step 1600: train loss 1.7669, val loss 1.8559\n",
            "step 1700: train loss 1.7447, val loss 1.8538\n",
            "step 1800: train loss 1.7229, val loss 1.8304\n",
            "step 1900: train loss 1.7059, val loss 1.8102\n",
            "step 2000: train loss 1.6847, val loss 1.8090\n",
            "step 2100: train loss 1.6605, val loss 1.7878\n",
            "step 2200: train loss 1.6433, val loss 1.7748\n",
            "step 2300: train loss 1.6454, val loss 1.7741\n",
            "step 2400: train loss 1.6258, val loss 1.7613\n",
            "step 2500: train loss 1.6219, val loss 1.7568\n",
            "step 2600: train loss 1.6048, val loss 1.7394\n",
            "step 2700: train loss 1.5929, val loss 1.7370\n",
            "step 2800: train loss 1.5838, val loss 1.7323\n",
            "step 2900: train loss 1.5697, val loss 1.7267\n",
            "step 3000: train loss 1.5599, val loss 1.7087\n",
            "step 3100: train loss 1.5546, val loss 1.7054\n",
            "step 3200: train loss 1.5529, val loss 1.7071\n",
            "step 3300: train loss 1.5372, val loss 1.6956\n",
            "step 3400: train loss 1.5232, val loss 1.7087\n",
            "step 3500: train loss 1.5215, val loss 1.6893\n",
            "step 3600: train loss 1.5234, val loss 1.6850\n",
            "step 3700: train loss 1.5051, val loss 1.6840\n",
            "step 3800: train loss 1.5104, val loss 1.6772\n",
            "step 3900: train loss 1.5086, val loss 1.6800\n",
            "step 4000: train loss 1.4909, val loss 1.6774\n",
            "step 4100: train loss 1.4872, val loss 1.6704\n",
            "step 4200: train loss 1.4813, val loss 1.6743\n",
            "step 4300: train loss 1.4716, val loss 1.6703\n",
            "step 4400: train loss 1.4600, val loss 1.6597\n",
            "step 4500: train loss 1.4666, val loss 1.6658\n",
            "step 4600: train loss 1.4621, val loss 1.6618\n",
            "step 4700: train loss 1.4664, val loss 1.6608\n",
            "step 4800: train loss 1.4496, val loss 1.6578\n",
            "step 4900: train loss 1.4486, val loss 1.6520\n",
            "step 5000: train loss 1.4358, val loss 1.6474\n",
            "step 5100: train loss 1.4374, val loss 1.6425\n",
            "step 5200: train loss 1.4236, val loss 1.6427\n",
            "step 5300: train loss 1.4257, val loss 1.6476\n",
            "step 5400: train loss 1.4347, val loss 1.6424\n",
            "step 5500: train loss 1.4167, val loss 1.6362\n",
            "step 5600: train loss 1.4181, val loss 1.6222\n",
            "step 5700: train loss 1.4054, val loss 1.6402\n",
            "step 5800: train loss 1.4206, val loss 1.6374\n",
            "step 5900: train loss 1.4035, val loss 1.6359\n",
            "step 6000: train loss 1.4015, val loss 1.6394\n",
            "step 6100: train loss 1.3908, val loss 1.6376\n",
            "step 6200: train loss 1.3929, val loss 1.6257\n",
            "step 6300: train loss 1.3931, val loss 1.6272\n",
            "step 6400: train loss 1.3818, val loss 1.6220\n",
            "step 6500: train loss 1.3859, val loss 1.6371\n",
            "step 6600: train loss 1.3782, val loss 1.6215\n",
            "step 6700: train loss 1.3741, val loss 1.6227\n",
            "step 6800: train loss 1.3776, val loss 1.6216\n",
            "step 6900: train loss 1.3732, val loss 1.6210\n",
            "step 7000: train loss 1.3717, val loss 1.6317\n",
            "step 7100: train loss 1.3698, val loss 1.6283\n",
            "step 7200: train loss 1.3672, val loss 1.6171\n",
            "step 7300: train loss 1.3622, val loss 1.6006\n",
            "step 7400: train loss 1.3658, val loss 1.6204\n",
            "step 7500: train loss 1.3642, val loss 1.6144\n",
            "step 7600: train loss 1.3421, val loss 1.6031\n",
            "step 7700: train loss 1.3555, val loss 1.6244\n",
            "step 7800: train loss 1.3444, val loss 1.6077\n",
            "step 7900: train loss 1.3414, val loss 1.6181\n",
            "step 8000: train loss 1.3478, val loss 1.6037\n",
            "step 8100: train loss 1.3435, val loss 1.6138\n",
            "step 8200: train loss 1.3474, val loss 1.6109\n",
            "step 8300: train loss 1.3289, val loss 1.6033\n",
            "step 8400: train loss 1.3368, val loss 1.6019\n",
            "step 8500: train loss 1.3313, val loss 1.6001\n",
            "step 8600: train loss 1.3324, val loss 1.5976\n",
            "step 8700: train loss 1.3338, val loss 1.5990\n",
            "step 8800: train loss 1.3226, val loss 1.5983\n",
            "step 8900: train loss 1.3235, val loss 1.6037\n",
            "step 9000: train loss 1.3156, val loss 1.5945\n",
            "step 9100: train loss 1.3130, val loss 1.5978\n",
            "step 9200: train loss 1.3225, val loss 1.5995\n",
            "step 9300: train loss 1.3117, val loss 1.6039\n",
            "step 9400: train loss 1.3181, val loss 1.6073\n",
            "step 9500: train loss 1.3184, val loss 1.5941\n",
            "step 9600: train loss 1.3135, val loss 1.5951\n",
            "step 9700: train loss 1.3012, val loss 1.5977\n",
            "step 9800: train loss 1.3090, val loss 1.5884\n",
            "step 9900: train loss 1.3031, val loss 1.5923\n",
            "step 9999: train loss 1.3114, val loss 1.5874\n",
            "\n",
            "Oh (Oh, yes you oh\n",
            "Whody you'ver stame 'Cause, no to if you some, I love love me thou're here's gun in ut take you hevere\n",
            "So what you say to don't feel to dinnan that yof win me,\n",
            "What I'm you minaste to try, sus\n",
            "That somented that, should?\n",
            "How that That she me move.-oh, m come ever\n",
            "You to love me, to me meatin ain'\n",
            "And forn iffight to 'cause pirlushice my hey?\n",
            "Youres crale you damer fornt plone us\n",
            "'t neverywords the searth you, I'm a lover sool\n",
            "Madly fext luse show wanna botthe seciter to jee all one art to be,\n",
            "I'm come don't rever\n",
            "TI could ease in it to heartht\n",
            "You cum theseen monnate shomebetwind the light slugace\n",
            "And like dmil, y girl ou that,me a cross me\n",
            "But sturnce to an\n",
            "You'd betters\n",
            "That's lone in' long the swry)\n",
            "I'll to so less love\n",
            "'Cause is gonna loveft your where streat danng nothie wim wime like it each car\n",
            "And every thes can\n",
            "Betwation\n",
            "Therer wake through\n",
            "There me to one feellingst\n",
            "Onless I need to girl\n",
            "Baby take (Ifave, I cally prep oneen be playsstlo be mine yey giry\n",
            "'Causese I gonna be take they's girst (Befor to trught\n",
            "Ooh (No, try, ply win I less\n",
            "Oh)\n",
            "Theme you comeb, an to ruedHeartion ther betclatients [Be as your here ('Cause ming ber boodaiour at way you, you of Wit you\n",
            "Hit'se of lonnathing the livore back a keep) I got it me\n",
            "I our pread you foor you love\n",
            "'Ca pplet\n",
            "Side makir ke my hit's night you smay lovet yours\n",
            "Be so us and closen, foolosecity,\n",
            "Am Jus flore to\n",
            "You're all monebody, it's sides surved\n",
            "Actre never\n",
            "Sincento prome\n",
            "Shidet ficuass theress evern yoing and tars you want I'm Not a know mand in to eac\n",
            "Seep living was ifrfeep oh say no sorrow\n",
            "Whoa, lose wac? You hado, It's yoru're hat lie meanie\n",
            "We fight the bree unly and, then your whis I could've time, finnato le my heave ya\n",
            "And I be mine\n",
            "That I thought you going you'd dext mo up\n",
            "We ever mayine timte\n",
            "Back yes a cant morboody,\n",
            "I'm think I go, Itawaid yeah yeah\n",
            "I'm to like you hare Yeah,\n",
            "I am this should I'll caust\n",
            "You nee with (I winnI's look you say I'm know yong\n",
            "I much you sumy doine m\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32\n",
        "block_size = 64\n",
        "max_iters = 10000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.5\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('bieber.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.7*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69sKaU4ZRGCG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNglAMr22Q50YBPBlYqEhg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}